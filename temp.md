# 프로젝트 Wrap Up

# Part 1 [팀] 프로젝트 Wrap Up

### 1️⃣ 프로젝트 개요

- Semantic Text Similarity (STS) Task
    - 두 문장을 입력받고 두 문장의 의미적 유사도를 점수로 나타내기
    - 두 문장이 의미가 유사할수록 높은 점수를 주고 그렇지 않을 수록 낮은 점수를 줘야함 (0점 ~ 5점 사이)
- 입력 및 출력 결과물
    - 입력 : 문장 쌍의 id, 문장 1, 문장 2, 주어진 문장의 유사도 점수
    - 출력 : 문장 쌍의 id, 평가 데이터에 있는 각 문장 쌍의 유사도 점수
- 환경
    - 팀 구성 및 컴퓨팅 환경 : 5인 1팀. 인당 V100 서버를 VS code와 SSH로 연결하여 사용
    - 협업 환경 : Notion, GitHub
    - 의사소통 : Slack, Zoom
- 프로젝트 구조 및 사용 데이터셋
    - 프로젝트 구조: Transformer 계열의 모델을 통해 구현한 Cross Encoder를 통해 각 문장의 유사도를 측정하고 그 값을 0에서 5사이의 값으로 스케일링 한 후 출력
    - 사용 데이터셋: 문장 짝과 그 문장 짝의 유사도를 가지고 있는 데이터셋을 활용하였고 각각 Train은 9324개, Valid는 550개, Test는 1100개의 데이터로 이루어져 있음

### 2️⃣ 프로젝트 팀 구성 및 역할

| 팀원 | 구성 및 역할 |
| --- | --- |
| 김주성 | Loss 함수 변경, Dropout 테스트, LR Scheduler 테스트 |
| 문지혜 | 모델 리서치, loss part 모델 성능 실험 |
| 이준범 | 데이터 증강, 하이퍼 파라미터 조정 |
| 정세연 | EDA, 데이터 전처리, 모델 성능 실험 |
| 홍찬우 | Cross validation, cosine scheduler, 모델 성능 실험 |

### 3️⃣ 프로젝트 수행 절차 및 방법

![스크린샷 2023-04-24 오전 10.02.27.png](%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20Wrap%20Up%2087ef1e0d09fd42f083e9e45e0e118e1d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_10.02.27.png)

- 세부적인 수행 과정 및 목록 : [프로젝트 수행 과정](https://www.notion.so/a43280acafb948dfb6172967985112d0)

### 4️⃣ 프로젝트 수행 결과

- 탐색적 분석 및 전처리 (학습데이터 소개)
    - 학습데이터 소개
        - 국민청원 게시판 제목 데이터, 네이버 영화 감성 분석 코퍼스, 업스테이지 슬랙 데이터에서 추출한 문장 사용
        - 두 문장과 문장 쌍에 대한 유사도 데이터 사용
    - 탐색적 분석
        - 데이터의 분포 확인
            
            ![스크린샷 2023-04-21 오후 4.44.22.png](%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20Wrap%20Up%2087ef1e0d09fd42f083e9e45e0e118e1d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-21_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_4.44.22.png)
            
        - Unknown Token 확인
            - 데이터의 문장을 직접 보았을 때, 사람의 이름에 해당하는 정보는 `<PERSON>`으로 가려 놓은 것을 확인할 수 있었음
            ex . “<PERSON> 님과 어제 저녁에 식사를 하였습니다!”
            - `<PERSON>`을 새로운 토큰으로 추가하여 `<PERSON>` 를 [UNK] 토큰이 아닌 special token으로 학습할 수 있게 하였음
        - re 라이브러리를 통한 데이터 전처리
            - 문장 내 특수 문자와 자음, 모음이 단독으로 반복되는 경우를 전처리
                - 필요 이상으로 반복되는 경우 3번 반복되는 형태로 통일시키는 방식으로 전처리를 수행하였음
        - 문장 위치 교환을 통한 데이터 증강
            - A, B 문장 짝을 B, A 문장 짝으로 바꾼 뒤 추가로 학습하는 것도 성능 향상에 도움이 될 것 같아 수행
            - 학습 데이터가 2배로 많아져서 학습에 2배의 시간이 걸렸지만 성능이 꽤나 향상되었음
        - 문장의 맞춤법 교정을 통한 데이터 전처리
            - 문장 내의 맞춤법 교정을 수행하면 같은 단어가 다르게 표기되어 생기는 문제점을 해결할 수 있을 것이라는 생각으로 맞춤법 교정을 수행
            - 큰 성능 상승을 가져오지는 못 했지만 소소한 성과는 거둘 수 있었음
        
- 모델 개요
    - ELECTRA
        - 2020년에 발표된 모델로 기존 BERT 계열의 모델들과 달리 대체 토큰 탐지라는 훈련 방식을 통해서 훈련을 하고 이를 통해서 ELECTRA는 기존의 모델들보다 더 적은 자원으로도 더 좋은 성능을 보여줌
    - RoBERTa
        - Dynamic masking 기법과 더 많은 데이터를 학습에 활용하여 BERT 모델을 더 강인하게 개선한 모델 (2019년 발표)
        - KLUE 벤치마크가 있어, 한국어 STS task에 적용하기 용이하다는 장점이 있음
    - ALBERT
        - 2019년에 발표된 모델로 factorized embedding parameterization와 cross-layer parameter sharing를 통해 BERT보다 훨씬 적은 parameter로 더 좋은 성능을 보여줌
- 모델 선정 및 분석
    - klue/RoBERTA_small & klue/RoBERTA_base & klue/RoBERTA_large
        - 프로젝트 초반, 한국어 NLP 벤치마크인 KLUE를 통해 모델 학습을 진행하였음.
        - 해당 모델을 기반으로 옵티마이저, loss 등 모델의 구조를 변경하고, 하이퍼파라미터를 조정하여 다양한 실험을 시도하였음
    - smartmind/albert-kor-base-tweak
        - 동일 조건으로 학습 시 klue/roberta보다 좋은 성능을 보이지 못해 사용하지 않음
    - snunlp/KR-ELECTRA-discriminator ✅
        - 프로젝트 후반, RoBERTa 모델이 일정 수준 이상으로 성능이 개선되지 않아 학습을 시도한 모델
        - 동일 조건으로 학습 하였을 때, 조사한 모델 중 가장 높은 성능을 달성함
    
    [모델 성능 평가 및 비교 페이지](https://www.notion.so/03f75c31bcab453f9d1105822faf932c)
    

- 모델 평가 및 개선 방법
    - 데이터 증강
        - Train Dataset과 Validation Dataset의 score label 불균형을 맞춰주기 위해 압도적으로 수가 많은 0점대 문장을 고정하고, 나머지 점수대의 sentence1, 2를 서로 바꿔 데이터 증강
            
            ![Untitled](%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20Wrap%20Up%2087ef1e0d09fd42f083e9e45e0e118e1d/Untitled%201.png)
            
    - 데이터 전처리 작업
        - Unknown Token 중 `<PERSON>` token을 corpus에 추가
        - 문장 내 특수 문자와 자음, 모음이 단독으로 반복되는 경우를 전처리
        - 맞춤법 오류 바로잡기
    - Loss function 변경
        1. L1 Loss를 MSE(L2) Loss로 변경
            - 사용하는 데이터가 0~5 사이로 범위가 정해져있기때문에 이상치에 강한 L1보다는 Regularization에 좋은 L2를 사용하는 것이 좋을 것 같아 사용했고 성능이 향상되었음
        2. BCE Loss, Contrastive Loss 등 다양한 loss function 적용
            - STS task에 적용 가능하거나, 성능 향상을 도모할 수 있는 loss 를 적용해 보았으나 성능 향상에 도움이 되지 않거나, 학습이 제대로 이루어지지 않음
    - Dropout
        - 모델의 과적합을 Dropout을 방지하기 위해 Dropout을 0.1로 적용하였더니 성능이 향상되었음, 하지만 Dropout의 비율이 높아지면 오히려 성능이 떨어지는 모습을 보였음
    - 앙상블
        - 서로 다른 조건에서 학습된 모델들의 앙상블을 통해서 성능 상승을 이루어냈음
        - 앙상블 결과를 구할 때, 평균값으로 값을 구하였는데 이때 단순히 각 모델 출력값들의 평균을 사용하는 것보다 각 모델들의 출력 값에 시그모이드 함수를 취한 후 평균을 내고 그 값에 다시 역 시그모이드 함수를 취하는 것이 더 좋은 결과를 낼 수 있을 것 같아 적용하였고 약간의 성능 상승을 이루어냈음

- 시연 결과
    - 모델 성능
        
        ![스크린샷 2023-04-24 오전 10.32.44.png](%E1%84%91%E1%85%B3%E1%84%85%E1%85%A9%E1%84%8C%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20Wrap%20Up%2087ef1e0d09fd42f083e9e45e0e118e1d/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2023-04-24_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%258C%25E1%2585%25A5%25E1%2586%25AB_10.32.44.png)
        
        최종 리더보드에서 0.9307이라는 결과를 얻었음
        

### 5️⃣ 자체 평가 의견

- 잘한 점들
    - EDA를 통해 데이터의 라벨 별 데이터 분포를 확인하고 이를 바탕으로 데이터 전처리 및 증강을 수행하였으며, 이 과정에서 다양한 자연어 데이터의 증강 기법을 알아보았다.
    - 가설을 세우고 검증하는 방식으로 프로젝트를 진행하였다.
    - 팀원 각자 가설을 검증한 내용을 팀에 공유를 잘해서 진행 상황을 알기 쉬웠다.
    - 대회 끝까지 최선을 다했다!!

- 시도 했으나 잘 되지 않았던 것들
    - LR_scheduler을 적용해 보았으나 성능 향상이 이루어지지 않음
        - pre-trained 모델의 optimal Scheduler을 확인하고 선택해야 함
        - optimal Scheduler을 적용한 경우에도 scheduler을 적용하지 않았을 때 더 성능이 높은 경우도 있었음
    - LR rate 변경
        - 다양한 LR rate를 테스트해보았는데 기존보다 성능이 좋아진 경우가 없었음
    - MSE Loss 이외의 다양한 loss function 사용
        - 문제의 출력을 0~1 사이의 값으로 변환해서 이진 분류 문제에 사용되는 BCE Loss를 사용하면 성능이 더 좋아지지 않을까 생각해서 시도했지만 성능이 나아지지 않았음
    - Data에서 영어, 특수 문자 제거
        - Data에 순수한 한글만 남아있으면 한국어 모델에서 더 좋은 성능을 보여줄 것이라 생각했지만 그렇지 않았음, 영어와 특수문자가 가지는 의미를 생각하지 않고 그냥 삭제해버려서 그런 것 같음
    - 한국어 문장을 영어로 번역해서 사용
        - 다양한 API를 사용해 번역을 시도했지만, 번역 성능이 너무 안 좋거나 시간이 오래 걸려 깊게 연구하지 못 했음
    - 문장 내 단어 순서 바꾸기를 통한 데이터 증강
        - 본래 문장의 의미가 훼손되므로 성능에 유의미한 변화가 없었다.

- 아쉬웠던 점들
    - 깃헙을 통한 코드 공유를 하지 않았다.
    - 역할 분담을 하기 보다, 각자 스스로 가설을 세우고 확인하는 식으로 했기에 협업이 부족했다.
    - 모델을 선정하고 테스트를 할 때 성능 향상에만 신경을 쓰고, 왜 잘되는지 고민이 부족했다.
    - 하나의 팀이라기보다는 5명의 개인이었다.

- 프로젝트를 통해 배운 점 또는 시사점
    - 프로젝트 계획을 구체적으로 짜고, 역할 분담을 명확하게 하여 효율적인 프로젝트 수행이 필요하다.
    프로젝트 로드맵을 세우고, 그 과정 안에서 각자 수행해야하는 내용(역할 분담)을 나누어 매번 진행상황을 공유해야함을 알게 되었다.
    - Pytorch Lightning 코드의 구조를 알고 baseline에 추가적인 기능을 구현할 수 있게 되었다.
    - Cross Validation(k-fold)의 개념을 정확히 이해하게 되었다.
    - 모델 선정과 데이터 전처리 및 증강이 성능 향상에 가장 중요하다는 것을 알게 되었다.
    - 가설을 많이 세우는 것보다 중요한 것 3~4개를 세우고 정확히 검증하는 것이 더 중요함을 깨달았다.
    - 단순히 논문을 보고 따라하는 것이 아니라 논문을 보며 어떻게 자신의 상황에 적용할 수 있을지를 고민해보아야 한다는 것을 알게 되었다.
    - EDA를 통해 데이터를 평가하고 데이터 전처리나 증강을 하는 것이 중요함을 깨달았다.
    - 베이스 라인 코드를 보며 데이터가 어떻게 모델에 입력되고 모델을 지나서 어떤 아웃풋이 나오고 그 아웃풋이 어떤 함수를 거쳐 score가 만들어지는지 볼 것 (파이프라인 어떻게 구성되어 있는지 보는 것!)
    - 가설을 세우고 이 가설을 검증할 실험을 한다! 가설을 세울 때, 팀원들과 같이 고민해보고 각자 서로 다른 방법을 실험해보는 것이 가장 효율적일 것이다!
    - 작은 모델로 가설을 검증하고 그 가설이 맞다고 판단되면 더 큰 모델에 적용된다! 낮에 가설검증, 실험하고 밤에는 GPU 돌려놓고 잠자고 일어나기
    - 다른 코드를 참고해도 되나, 성능이 왜 좋게 나오는지에 대해 깊게 고민해야 됨! 그리고 참고한 코드에서 더 나아가 보기!

---

# Part 2 [개인] 개인회고

## 김주성

**학습 목표 달성을 위한 나의 노력**

이번 학습에서 저의 목표는 제가 이해할 수 있는 방법으로 성능을 향상시키자였습니다. 처음에는 단순히 성능을 올리는 것이 저의 목표였지만 멘토님께 이해하지 못한 채로 단순히 성능을 올리는 것은 아무 의미가 없다고 하는 말을 듣고 저의 목표를 수정하게 되었습니다. 프로젝트 과정에서 저는 제가 이해하지 못하는 방식으로 성능을 올리려는 시도를 최대한 피하였습니다. 논문을 참고할 때도 단순히 어떤 방식을 썼더니 성능이 잘 나왔다는 것에만 집중하지 않고 그 방식의 어떤 특성 때문에 성능이 올라가는 것일지를 생각해보고 또 그 특성이 제가 진행하는 프로젝트에 도움이 될 수 있을지를 생각해보았습니다. 이러한 프로젝트 진행 방식이 지금 당장은 좋은 결과를 내지 못 할 수도 있지만 시간이 지난 후에는 훨씬 더 좋은 결과를 낼 수 있을 것이라고 생각합니다.

**내가 모델을 개선한 방법**

저는 모델을 개선하기 위해 이 태스크에 맞는 모델이 어떤 모델이 있는지를 찾아보고 그 모델의 논문이나 인터넷 자료들을 보며 코드를 어떤 식으로 수정하면 좋을지 생각해보았습니다. 예를 하나 들어보자면 Loss 함수를 교체하는 방법을 사용했던 일이 있습니다. 제가 사용하는 모델의 논문에서는 L2 loss를 사용하는데 코드에서는 L1 loss를 사용하고 있었습니다. 이때 2 loss 함수의 특성을 알아보고 이번 프로젝트에서 사용하는 데이터의 특성을 고려해보면 L2 loss가 좀 더 적합할 것 같다는 판단이 들어 loss를 교체해보았고 성능을 올릴 수 있었습니다. 이외에도 lr 변경, dropout 사용, 데이터 증강 등의 기법 등을 사용해보았고 그중 일부는 유의미한 성능 향상을 보여주었습니다. 

**내가 한 행동의 결과와 그를 통해 알게된 점**

저는 프로젝트 초기에는 논문과 비슷하게 설정하면 성능이 보장될 것으로 생각하였으나 성능이 생각보다 잘 나오지 않았습니다. 그 과정을 거치면서 논문에서의 상황과 저의 상황이 다른 데 무작정 논문을 따라 하는 것은 좋은 방법이 아닐 것이라는 생각이 들었고 앞으로는 논문은 참고 자료로만 사용하고 저의 상황에 맞는 방법을 찾아보며 프로젝트를 진행하는 것이 좋을 것 같다고 생각하게 되었습니다. 또한 팀으로 프로젝트를 진행하며 프로젝트 관리의 중요성에 대해서도 알게 되었습니다. 이번 프로젝트를 수행할 때 서로 간에 얻은 실험 결과를 공유하며 팀으로 함께 프로젝트를 진행하기는 했지만 체계적으로 프로젝트 관리를 한 것은 아니었습니다. 그로 인한 결과로 서로 간에 정보가 제대로 전달되지 않거나 비슷한 실험을 하는 등의 문제점이 발생했었고 이는 프로젝트 후반부 시간 부족으로 이어졌습니다. 팀으로서 프로젝트를 진행해본 경험이 거의 없었기 때문에 발생했다고 생각을 하고 이후 프로젝트부터는 팀원과 함께 프로젝트 진행 상황을 체계적으로 관리해가며 진행한다면 더 좋은 결과를 얻을 수 있을 거라고 생각이 듭니다.

**내가 마주했던 한계와 아쉬웠던 점**

저는 프로젝트 경험이 많지 않았고 또 이렇게 한정된 데이터에서 모델의 성능을 최대한 끌어올리는 프로젝트는 진행해본 적이 없었기 때문에 많은 어려움이 있었습니다. 처음 프로젝트 시작에서부터 어떤 방향으로 프로젝트를 진행해야 할지 갈피를 제대로 잡지 못한 채 이것저것 건드려보며 시간을 비효율적으로 보냈고 그로 인해 후반부에 프로젝트의 방향성을 어느 정도 잡은 후에 시간 부족에 시달려야 했고 이 때문에 모델의 성능을 충분히 올리지 못한 것 같아서 아쉬움이 많이 들었습니다.

**다음에 시도해보고 싶은 것**

이번 프로젝트에서는 시간 관리의 실패로 어려움을 겪었기 때문에 다음 프로젝트부터는 팀원 간의 소통을 통해서 프로젝트의 방향성을 잡은 다음에 효율적으로 진행을 해보고 싶습니다. 기업에서 진행되는 프로젝트처럼 체계적으로 관리가 되지는 않겠지만 분명히 프로젝트 진행에 도움이 될 것 같고 이러한 프로젝트 관리 능력은 현업에 가서도 꼭 필요한 능력이라고 생각되어서 꼭 진행해보고 싶습니다.

## 문지혜

✔️ 나의 학습 목표

1. STS task 파악하기
2. 해당 task를 학습하기 위해 필요한 데이터의 특징 알아보기 & 주어진 데이터의 특성 알아내기
3. 학습 모델의 구조를 파악하고, 학습 성능을 개선해보기

✔️ 학습 목표를 달성하기 위해 내가 하고자 한 것

1. 주어진 코드와 데이터, 하고자 하는 task를 파악하기
    - `train.py` 와 `inference.py` 코드 분석
    - `train.csv`, `dev.csv`, `test.csv` 에 있는 문장 쌍 데이터 분석
    - STS task SOTA 모델 리서치 → BERT, RoBERTa, ELECTRA 등
2. RoBERTa-small, base, large 학습
    - 우리가 하고자 하는 task가 한글 NLP이므로, KLUE 벤치마크를 통해 학습을 진행해 보면 좋을 것 같다는 생각을 함.
    - 주어진 학습 코드에도 디폴트 값으로 `klue/roberta-base` 가 지정되어 있었음. 이를 기반으로 optimizer, dropout 등을 적용하고 loss function을 변경하여 모델 튜닝을 시도하였음.
    - optimizer 는 ADAMW를 사용하였고, dropout 값은 0.1로 설정함
3. loss function 교체
    - L1 Loss → MSE Loss → Contrastive Loss (구현했지만 학습이 제대로 이루어지지 않음) → BCE Loss → MSE Loss ✅
4. 모델 바꾸기 RoBERTa → ELECTRA
    - 모델 튜닝으로는 성능 개선에 한계가 있는 것으로 보여, RoBERTa 이외의 다른 모델도 실험을 해봄. 그 중, 팀원이 ELECTRA 모델의 성능을 공유해서 대회 후반부에는 ELECTRA 모델로 학습을 진행함.
5. 데이터 증강
    - 주어진 데이터를 증강하기 위해 문장 1과 문장 2의 순서를 바꾼 데이터를 training 데이터에 추가하여 실험을 진행함.
    - 또한, 라벨의 균형을 맞추고자, 1~5 사이의 라벨을 가지는 데이터만을 위와 같은 방법으로 증강함.

✔️ 다음 프로젝트에서 시도해 보고 싶은 것

- [데이터 부분] 언어 데이터를 다루는 경험이 처음이라, 어떻게 데이터를 전처리하고 데이터를 증강시키는지 몰랐었는데, 이번 대회 경험을 통해 언어 데이터의 다양한 전처리 기법과 증강 기법에 대해 알게되었다. 아쉬운 점은 이번 대회에서는 이렇게 알게 된 사실은 많지만 실제로 적용해 보지는 못했다는 것이다. **다음 프로젝트 데이터에 더 집중하고 알게 된 사실을 실제로 적용해볼 것이다.**
- [모델 학습 부분] 너무 처음부터 하나의 모델에만 매몰되어, 다양한 모델을 학습하는 것을 빠르게 시도하지 않았다는 것이다. 데이터, 모델, 모델튜닝 이러한 순서로 성능 개선이 더 잘 이루어 진다는 것을 깜박하고, 반대로 역행해버렸다. 이걸 대회 다 끝나고 이제서야 인지하게 되었다. **다음 프로젝트에는 데이터부터 꼼꼼히 살펴보고 대회 초반에 여러 모델을 학습하고, 후보 모델을 골라 모델을 튜닝하는 방식으로 진행해야 겠다!!**

## 이준범

학습 목표

- 팀 목표 : ‘중위권만 하자.’
- 나의 목표:   베이스 라인 코드가 어떤 파이프 라인을 통해 작동하는지 알기

배운 점

- 멘토님의 말씀을 듣고 목표를 잡았다. “베이스 라인 코드가 어떻게 작동하는지 알기만 해도 성공이다.”
- VScode에 원격 서버를 연결하고 환경 세팅하는 법을 배웠다.
- 파이프 라인을 알기 위해 메모장에 코드 설명을 적었다.
- Wandb를 이용해서 학습 결과를 시각화했다.
- 여러가지 모델과 하이퍼 파라미터(학습률, 배치 사이즈, 에포크 수) 를 바꿔가며 pearson을 비교했다. 학습률 스케줄러를 사용했다.
- 역번역과 문장 내 단어 재배치를 통해 데이터 증강을 시도했다.
- 오버피팅을 방지하기 위해 drop out과 Early stopping을 사용했다.

마주한 한계는 무엇이며, 아쉬웠던 점은 무엇인가?

- 하이퍼 파라미터를 수동으로 바꾸다보니 최적의 하이퍼 파라미터를 찾지 못했다.
- EDA(Exploratory Data Analysis, 탐색적 데이터 분석)을 전혀 하지 못했다.
- 학습한 모델을 AI stages에 한번도 제출하지 못했다.(나 진짜 뭐했지?)
- 데이터 전처리 기법과 데이터 증강기법이 더 있음에도 제대로 시도하지 못했다.
- 베이스라인 코드를 바꾸지 못한 것이 아쉽다. 파이토치 라이트닝 구조를 더 공부해야겠다.
- K-fold를 해보지 못했다.
- 깃헙을 사용하지 않았다.
- GPU 서버를 충분히 활용하지 않고 밤에 쉬게 했다.

한계/교훈을 바탕으로 다음 프로젝트에서 시도해보고 싶은 점은 무엇인가?

- 다음 프로젝트 시작 전, 파이토치와, 파이토치 라이트닝 구조 설명을 내 블로그에 적고 싶다.
- 대회를 시작하기 전에, 계획을 세우고 시작하자!
- 데이터 EDA를 하고 학습을 시작하자.
- Wandb Sweep을 사용해서 최적의 하이퍼 파라미터를 찾자.
- K-Fold를 해보자!
- 모르는 내용이 있을 때마다, 검색을 하고 이해를 하면, 간단하게 노션에 적어놓자. 같은 내용을 반복해서 찾는 시간을 줄이자.
- 깃헙 사용법을 익혀서 팀원들과 같이 협업하자.
- 낮에 작은 모델을 돌리며 확인을 하고 밤에 자기 전에 학습을 돌려 놓고 자서 GPU를 충분히 활용하자.

나는 어떤 방식으로 모델을 개선했는가?

- 이번 프로젝트에서 나는 모델 개선에서 실패했다.

내가 해본 시도 중 어떠한 실패를 경험했는가? 실패의 과정에서 어떠한 교훈을
얻었는가?

- 학습률을 변경 또는 학습률 스케줄러를 사용하니 오히려 Train loss가 발산하고 Pearson도 떨어졌다.
    - 해당 모델에 대해 이해가 필요하다. 모델의 기본 하이퍼 파라미터값을 찾고, 거기서 시작하자.
    - Wandb sweep을 사용하자.
- 모델에 대한 공부 없이 성능만 잘나오면 그것으로 결정했다.
    - 해당 모델이 어떤 데이터를 바탕으로 학습을 했고 어떤 Task에 적합한지 찾자.
- 데이터 전처리를 전혀 시도하지 않았다.
    - 다음에는 EDA를 통해 데이터 전처리를 어떻게 할지를 고민하자.

협업 과정에서 잘된 점/ 아쉬웠던 점은 어떤 점이 있는가?

- 잘된 점
    - 노션에 리더보드를 만들어 실시간으로 다른 사람의 모델명, 하이퍼 파라미터 등을 알 수 있었다.
    - 팀원들이 모두 커뮤니케이션에 적극적이고, 다른 사람의 의견을 잘 받아들였다.
- 아쉬웠던 점
    - 깃헙을 사용하지 않았다. 다른 사람의 코드를 직접 확인하지 못하고 설명만 듣는 경우가 많았다.
    - 정해진 역할이 없었다. 전처리-모델-옵티마이저-손실함수 등 정해진 구역에서만 했으면 좀더 높은 순위에 오르지 않았을까 싶다. 다만, 정해진 역할이 없어 모든 부분을 스스로 공부했던 것은 좋았다.
    - 내가 팀에 기여한 부분이 없었다. 환경 세팅하고 파이프 라인을 이해하는데 시간을 많이 소모했고, 피어세션 시간에 내가 모르는 용어가 너무 많아 대화에 참여하지 못했다.
        - 모르는 게 있을 때 그냥 질문하자! 모르는 걸 숨길 수록 스트레스 받는다.

## 정세연

**학습 목표**

- STS task를 이해하고 프로젝트 수행하기
- Pytorch Lightning 코드의 구조를 이해하고 추가적인 기능을 어떻게 구현하는지 익숙해지기
- 자연어 데이터 증강 기법을 알고 이해하기

**모델 개선 방법**

1. 모델 탐색 : Encoder 기반 모델 중 현재의 문제에서 좋은 성능을 보이는 모델을 탐색
    - xlm-roberta-base 모델을 실험. 다국어 문제를 위한 모델로, 데이터 증강에서 문장을 영어로 번역하여 활용할 때 성능 개선에 도움이 될 것이라 생각하였으나 낮은 성능으로 폐기
    - snunlp/KR-ELECTRA-discriminator 모델 실험. KLUE-roberta 모델을 활용하여 프로젝트를 수행 중, 모델의 한계를 느끼고 다른 모델을 탐색하다가 electra 라인의 모델 중 snunlp/KR-ELECTRA-discriminator 모델을 수행하고 좋은 성능을 보여 팀의 기본 모델로 선정
- 학습 및 검증 데이터의 분포를 분석하고, EDA를 수행하여 데이터 증강을 통해 데이터 라벨 별 분포의 차이를 줄여서 학습하였음
    - EDA를 통해 모델의 성능 개선을 위한 아이디어를 제안하고, 직접적인 수행은 다른 팀원과 역할을 나누어 수행함
    - train data와 validation data의 분포가 차이를 보임 → 이를 개선하기 위해 total (train+valid) data의 분포를 파악하여 이를 최대한 유지하는 방향으로 train, valid data split을 수행
- 데이터 내에서 반복적으로 등장하는 문자인 `<PERSON>`을 확인하고 이에 대한 전처리 작업을 수행하였음
    - 소셜네트워크 상에서 수집한 문장 데이터에서 사람의 이름과 같은 가려야할 정보가 포함된 경우 이를 `<PERSON>`으로 마스킹하였음
    - `<PERSON>`의 경우 vocab에 없는 단어로 모두 `[UNK]` 토큰으로 처리함.
    - `[UNK]` token이 너무 많이 등장하는 경우 학습에 방해가 될 뿐만 아니라 `<PERSON>`은 어떤 특수한 의미를 갖는 단어로 처리해줘야한다고 생각하였음
    - `<PERSON>` token을 tokenizer의 corpus에 추가하여 처리함

**알게된 점**

- 다양한 자연어 데이터 증강 기법을 알게 되었음
    - KoEDA, 번역, 맞춤법, chatGPT에게 의미가 같은 문장 만들어달라고 하기
    - 증강 데이터가 원본 데이터에 비해 너무 많은 것은 문제가 될 수 있음
    - 증강된 데이터의 유효성을 평가하고 이를 바탕으로 학습에 활용하는 작업이 반드시 포함되어야 함
- 프로젝트를 진행할 때에 중요한 점을 알게 되었음
    - 프로젝트 로드맵. 구체적인 계획의 중요성 → 전체 계획이 있어야 헤매지 않는다
    - 팀단위 프로젝트 관리 경험의 중요성 → github 초보는 팀 repository 활용이 어렵다..🥲
    - private & public 리더보드는 어떤 결과로 나타날지 아무도 모른다..! 어떤 모델을 최종으로 선택할지는 일반화 성능을 고려해서 선택한다. (그래도 결과는 모른다.)
    - 역할 분담의 중요성. 각개전투보다는 역할을 나누어 효율적으로 수행하기
- Cross validation(K-fold)의 개념 및 활용 방식
    - train + validation 데이터를 합친 total dataset에서 k-fold를 하는 경우
    - 1개를 검증 데이터로 떼어 놓고, 나머지 k-1개로 cross validation을 수행함
    - 1개의 검증 데이터는 모델의 성능 평가에서 활용하여 학습을 중단할지 계속할지 등을 판단하는 척도로 활용
- 가설 검증의 방법에 대해 알게 되었음
    - 가설을 세우고 검증을 할 때 한번만 수행하면 안됨. init parameter 값에 따라서도 성능은 바뀜
    - 가설을 세우고 여러 seed 값으로 수행해보고 평균 점수를 내기. 이를 통해 신뢰할 수 있는 가설의 유효성 평가 결과를 얻을 수 있음

**한계 & 아쉬운 점**

- 프로젝트 시작 단계에서 프로젝트 기간을 고려해서 전체적인 로드맵을 그려놓고 그 안에서 task를 수행하는 것이 더 좋았을 것 같다
    - 프로젝트 전체적인 로드맵이 없으니, 길을 잃었을 때 한 없이 헤매게 됨
    - 프로젝트 마지막 쯤에는 시간이 부족해서 수행하지 못하는 것도 많았음
- github를 적절히 활용하지 못했음
    - 템플릿을 정하고 프로젝트 코드를 팀원과 관리하기 위해 github를 적절히 활용했다면 이후 모델 개선 및 관리에 더 도움이 되었을 것이라 생각함
- 역할 분배를 통해 효율적인 작업을 해야하는데, 이 부분이 서툴러서 각개전투하며 프로젝트를 수행하였음.
- Cross Validation(K-fold)의 개념의 혼동으로 프로젝트 기간 내에 적절히 활용하지 못했음(다른 팀원 분이 수행하셨다!)
- wadb의 sweep 을 적절히 활용하지 못했음
    - hyper parameter tuning은 전체적인 모델의 구조 및 기능을 확립한 후에 수행하려 하였으나 시간상의 문제로 수행하지 못했음
    - 프로젝트 기간 이후 sweep 코드를 프로젝트 base 코드에 추가해볼 예정
- 기본 코드의 전체적인 구조는 유지한 채 가설 검증을 위해 필요한 기능을 추가하는 수준으로만 수행하였음
    - dual encoder 구현과 같이 전체 코드 구조를 고쳐야하는 경우 수행하지 못하였음
- 다양한 데이터 증강 기법을 활용하였지만 증강시킨 데이터의 유효성을 검증하는 작업은 생각하지 못했음
- 가설 검증에서 학습을 여러번 수행하여 initialized value 에 따른 성능 차이를 고려하여 가설이 유용한지를 판단해야하는데 가설 검증 단계에서 한번씩 수행해보고 바로 넘어 갔음
- 잘되면 왜 잘되는지, 안되면 왜 안되는지를 잘 생각하지 않고, 가설을 세우고 이를 검증했을 때 성능이 좋으면 쓰고 안좋으면 버리는 식으로만 프로젝트를 수행하였음

**다음에 시도해보고 싶은 것**

- 데이터 증강을 한 후 해당 데이터의 유효성을 검증하는 단계를 추가하여 수행하기 → 이를 통해 어떤 데이터 증강 기법이 이 문제를 해결하는데 도움이 되었는지를 더 명확히 파악할 수 있을 것이다.
- 프로젝트 전체적인 로드맵을 설정하기
    - 모델 탐색 → 모델 커스텀(optimizer, loss, scheduler, checkpoint 등) → 데이터 전처리 및 증강 → hyperparameter 튜닝을 큰 흐름으로 생각하고 수행해보자
    - 모델 탐색에서는 후보 모델들을 팀원과 역할을 나누어 수행해보기
    - 모델 커스텀, 데이터 전처리 및 증강, hyperparameter 튜닝은 절차적이라기 보다는 역할 분담을 통해 동시다발적으로 수행하기
    - 하지만 큰 흐름을 위에처럼 생각하고 프로젝트 기간 중 각 task에 대해 어느정도 시간을 투자할 지를 고려하며 프로젝트를 수행하기
- github으로 협업 및 팀 프로젝트 코드  관리하기
    - 프로젝트 시작할 때 팀이 정한 템플릿에 맞게 기본 코드 수정하기. 템플릿에 맞게 새로운 기능 구현하기

## 홍찬우

### 시작

제대로 된 대회 경험을 가진 팀원이 없어서 초반엔 어떻게 진행해야 할지 쉽지 않았습니다. 배운 것들을 잘 활용하자는 마음으로 4~5주차에 배운 깃허브와 노션을 잘 활용해 협업할 것을 목표로 했습니다. 

또한 프로젝트 수행 방향을 모델 선정 → 모델 커스텀 → data EDA & 전처리 → 하이퍼파라미터 튜닝 → 앙상블 순서로 정했습니다. 이 순서 안에서 각자 다양한 가설을 설정하고, 검증하기로 했습니다.

### 개선

1) roberta 논문에 나와있는 cosine warmup scheduler를 klue/roberta-large 모델에 적용

- 성능을 크게 향상시키진 못했습니다. scheduler의 역할과 장점에 대해선 알게 됐지만, 모델 성능 향상에 잘 적용하는 것은 쉽지 않았습니다.

2) 한국어 문장을 영어로 번역 후 roberta-base 모델에 적용

- 다양한 api를 이용해 번역을 시도했으나, 번역 성능이 너무 좋지 않거나 시간이 오래 걸려 중간에 포기했습니다. 잘 번역된 문장은 성능 향상에 도움이 될 것이라고 기대했는데, 끝까지 검증하지 못해 아쉬웠습니다.

3) 미션 코드를 바탕으로 K-Fold cross validation 적용

- train 성능은 높게 나왔으나 실제 제출했을 때 성능은 현저히 낮았습니다. 미션 코드는 한 모델을 계속 학습시키며 과적합을 초래했고, 이는 적절하지 않다고 판단해 그 후 적용은 다른 방향으로 시도했습니다.

4) data augmentation 후 snunlp/electra-discriminator 모델에 적용

- 팀원의 EDA를 바탕으로 label 분포를 탐색 후 모든 label data를 균등하게 증강시켰습니다. 이를 새로 발견한 electra 모델에 적용하고, 성능이 매우 많이 향상됐습니다.

5) data augmentation과 K-fold cross validation 적용

- 이전 roberta 모델에 적용한 K-Fold CV와 다르게 Fold마다 모델을 새로 만들었고, 이를 평균을 내어 사용했습니다. 과적합을 방지했으며 성능도 나쁘지 않았습니다.

6) 다양한 하이퍼파라미터 조정

- 모델 안에서 batch-size, epoch와 같은 하이퍼파라미터를 수정해가며 실험했습니다. 모델 변경 및 데이터 증강과 비교했을 땐 성능에 큰 영향을 미치지 못했습니다.
- learning rate이 확실히 중요하다는 것을 알 수 있었습니다. 적절한 learning rate을 적용하지 않으면 학습이 제대로 이루어지지 않는 것을 몸으로 경험할 수 있었습니다.

### 느낀 점

프로젝트 초반에 모델을 klue/roberta로 고정하고 너무 많은 시간을 소요했으며, 데이터 자체보다는 모델 안에서 customize 하는 방향으로 깊게 생각했습니다. 이번 프로젝트를 통해 모델 선택과 데이터가 가장 중요한 것을 알게 됐습니다. 또한 깃허브를 활용하지 못하고 프로젝트를 진행한 점이 아쉬웠습니다. 

이를 바탕으로 추후 프로젝트에선 깃허브 협업을 바탕으로 모델 선정, 데이터 탐색 및 증강을 우선으로 실험하고 그 후 세밀한 customize를 시도하려 합니다.

---

### 멘토링 팁

- 베이스 라인 코드를 보며 데이터가 어떻게 모델에 입력되고 모델을 지나서 어떤 아웃풋이 나오고 그 아웃풋이 어떤 함수를 거쳐 score가 만들어지는지 볼 것 (파이프라인 어떻게 구성되어 있는지 보는 것!)
- 가설을 세우고 이 가설을 검증할 실험을 한다! 가설을 세울 때, 팀원들과 같이 고민해보고 각자 서로 다른 방법을 실험해보는 것이 가장 효율적일 것이다!
- 작은 모델로 가설을 검증하고 그 가설이 맞다고 판단되면 더 큰 모델에 적용된다! 낮에 가설검증, 실험하고 밤에는 GPU 돌려놓고 잠자고 일어나기
- 다른 코드를 참고해도 되나, 성능이 왜 좋게 나오는지에 대해 깊게 고민해야 됨! 그리고 참고한 코드에서 더 나아가 보기!
- dual encoder, cross encoder : sts 에서 두 문장 사이에 어떻게 시밀러리티를 구할 것인가에 대한 문제!
- klue-roberta-large가 아마 제일 잘 될 것!
